{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e64624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from ultralytics import RTDETR\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fba5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU) is available and set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") # Use the first GPU\n",
    "    print(f\"Training on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Training on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117e5880",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84450a42",
   "metadata": {},
   "source": [
    "The modifications to the network architecture were made with the goal of creating a lightweight model specifically optimized for \"shrimp fry\" detection, aiming to improve performance in terms of speed and resource usage. Here's a breakdown of the reasons for each change:\n",
    "\n",
    "1.  **`nc: 1` (Number of Classes)**:\n",
    "    *   The original `nc: 80` indicated the model was designed for 80 different object classes (e.g., COCO dataset).\n",
    "    *   For \"shrimp fry\" detection, we are only interested in detecting a single class. Changing `nc` to `1` tailors the model's output layer to predict only one type of object, significantly reducing the complexity of the final detection head and making it more efficient for this specific task.\n",
    "\n",
    "2.  **`scales: s: [0.33, 0.25, 256]` (Model Scaling Constants)**:\n",
    "    *   The original `l: [1.00, 1.00, 1024]` corresponded to a \"large\" model with high depth, width, and maximum channels.\n",
    "    *   By changing to `s: [0.33, 0.25, 256]`, we are adopting a \"small\" scale. This means:\n",
    "        *   `0.33` (depth multiplier): Reduces the number of layers or repeats in the network blocks.\n",
    "        *   `0.25` (width multiplier): Reduces the number of channels (feature map dimensions) in the network.\n",
    "        *   `256` (max_channels): Sets an upper limit on the number of channels, further constraining the model's size.\n",
    "    *   These reductions collectively make the model significantly smaller, faster to train, and quicker at inference, which is crucial for real-time applications or deployment on edge devices.\n",
    "\n",
    "3.  **`backbone` and `head` layers (Reduced Repeats and Channels)**:\n",
    "    *   **Reduced `repeats`**: In modules like `HGBlock` and `RepC3`, the `repeats` parameter (e.g., from `6` to `2` or `1`) was decreased. This directly reduces the number of times a particular block structure is repeated, leading to a shallower network with fewer computations.\n",
    "    *   **Reduced `channels`**: The number of output channels in various `Conv`, `HGStem`, `HGBlock`, and `RepC3` layers was substantially lowered (e.g., `HGStem` from `[32, 48]` to `[8, 12]`, `Conv` in the head from `[256, 1, 1]` to `[64, 1, 1]`). Fewer channels mean fewer feature maps are processed at each stage, which drastically cuts down on the model's parameter count and computational load.\n",
    "    *   **Overall Impact**: These changes across both the feature extraction (backbone) and detection (head) parts of the network result in a much more compact and efficient model. While a larger model might capture more general features, for a specialized task like shrimp fry detection, a smaller, optimized model can often achieve comparable or even better performance with significantly less computational overhead, making it more practical for deployment.\n",
    "\n",
    "These modifications collectively aim to create a highly efficient and performant model for the specific task of detecting shrimp fry, balancing detection accuracy with computational cost.\n",
    "\n",
    "<task_progress>\n",
    "- [x] Analyze the current `rtdetr-l.yaml` architecture.\n",
    "- [x] Determine appropriate `depth` and `width` reductions for a lightweight model.\n",
    "- [x] Update the `scales` parameter to reflect the lightweight architecture.\n",
    "- [x] Adjust `nc` to 1 for shrimp fry detection.\n",
    "- [x] Modify `backbone` and `head` layers to reduce complexity (channels, repeats).\n",
    "- [x] Apply changes to `Lib/site-packages/ultralytics/cfg/models/rt-detr/rtdetr-l.yaml`.\n",
    "- [x] Confirm the changes.\n",
    "</task_progress>\n",
    "</attempt_completion>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba79ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RTDETR('rtdetr-shrimp-s.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0d421de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.203  Python-3.13.7 torch-2.7.1+cu118 CUDA:0 (NVIDIA GeForce RTX 2060 SUPER, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=Shrimp-larvae-detection-1/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=rtdetr-shrimp-s.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=train8, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=E:\\PD1ModelTrainings\\PD1ModelTrainingCodes\\runs\\detect\\train8, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1800  ultralytics.nn.modules.block.HGStem          [3, 8, 12]                    \n",
      "  1                  -1  2      3824  ultralytics.nn.modules.block.HGBlock         [12, 12, 32, 3, 2]            \n",
      "  2                  -1  1       352  ultralytics.nn.modules.conv.DWConv           [32, 32, 3, 2, 1, False]      \n",
      "  3                  -1  2     25888  ultralytics.nn.modules.block.HGBlock         [32, 24, 128, 3, 2]           \n",
      "  4                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  5                  -1  2     73440  ultralytics.nn.modules.block.HGBlock         [128, 48, 256, 5, 2, True, False]\n",
      "  6                  -1  2     95968  ultralytics.nn.modules.block.HGBlock         [256, 48, 256, 5, 2, True, True]\n",
      "  7                  -1  2     95968  ultralytics.nn.modules.block.HGBlock         [256, 48, 256, 5, 2, True, True]\n",
      "  8                  -1  1      2816  ultralytics.nn.modules.conv.DWConv           [256, 256, 3, 2, 1, False]    \n",
      "  9                  -1  2    286656  ultralytics.nn.modules.block.HGBlock         [256, 96, 512, 5, 2, True, False]\n",
      " 10                  -1  1     32896  ultralytics.nn.modules.conv.Conv             [512, 64, 1, 1, None, 1, 1, False]\n",
      " 11                  -1  1     49984  ultralytics.nn.modules.transformer.AIFI      [64, 256, 8]                  \n",
      " 12                  -1  1      4224  ultralytics.nn.modules.conv.Conv             [64, 64, 1, 1]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14                   7  1     16512  ultralytics.nn.modules.conv.Conv             [256, 64, 1, 1, None, 1, 1, False]\n",
      " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     57856  ultralytics.nn.modules.block.RepC3           [128, 64, 1]                  \n",
      " 17                  -1  1      4224  ultralytics.nn.modules.conv.Conv             [64, 64, 1, 1]                \n",
      " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 19                   3  1      8320  ultralytics.nn.modules.conv.Conv             [128, 64, 1, 1, None, 1, 1, False]\n",
      " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1     57856  ultralytics.nn.modules.block.RepC3           [128, 64, 1]                  \n",
      " 22                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 24                  -1  1     57856  ultralytics.nn.modules.block.RepC3           [128, 64, 1]                  \n",
      " 25                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  1     57856  ultralytics.nn.modules.block.RepC3           [128, 64, 1]                  \n",
      " 28        [21, 24, 27]  1   7156451  ultralytics.nn.modules.head.RTDETRDecoder    [1, [64, 64, 64]]             \n",
      "rtdetr-shrimp-s summary: 313 layers, 8,166,139 parameters, 8,166,139 gradients, 14.8 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 82.652.8 MB/s, size: 35.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\PD1ModelTrainings\\PD1ModelTrainingCodes\\Shrimp-larvae-detection-1\\train\\labels.cache... 4140 images, 35 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 4140/4140 4.4Mit/s 0.0s0s\n",
      "WARNING Box and segment counts should be equal, but got len(segments) = 4397, len(boxes) = 143573. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.40.2 ms, read: 30.620.5 MB/s, size: 23.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\PD1ModelTrainings\\PD1ModelTrainingCodes\\Shrimp-larvae-detection-1\\valid\\labels.cache... 386 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 386/386 300.2Kit/s 0.0s\n",
      "WARNING Box and segment counts should be equal, but got len(segments) = 407, len(boxes) = 13790. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "Plotting labels to E:\\PD1ModelTrainings\\PD1ModelTrainingCodes\\runs\\detect\\train8\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 87 weight(decay=0.0), 150 weight(decay=0.0005), 170 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mE:\\PD1ModelTrainings\\PD1ModelTrainingCodes\\runs\\detect\\train8\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n",
      "\u001b[K       1/30      6.06G      2.353     0.3345     0.7073        655        640: 0% ──────────── 0/259  3.6s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PD1ModelTrainings\\DLenv\\Lib\\site-packages\\torch\\autograd\\graph.py:824: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:97.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K       1/30      9.28G      2.146     0.1747     0.6841        575        640: 15% ━╸────────── 40/259 0.6it/s 1:46<6:38\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR_HOST_ALLOCATION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mShrimp-larvae-detection-1/data.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PD1ModelTrainings\\DLenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:800\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    797\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    798\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PD1ModelTrainings\\DLenv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:235\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    232\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PD1ModelTrainings\\DLenv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:432\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28mself\u001b[39m.tloss = (\n\u001b[32m    428\u001b[39m         (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss_items\n\u001b[32m    429\u001b[39m     )\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PD1ModelTrainings\\DLenv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PD1ModelTrainings\\DLenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PD1ModelTrainings\\DLenv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR_HOST_ALLOCATION_FAILED"
     ]
    }
   ],
   "source": [
    "result = model.train(\n",
    "    data='Shrimp-larvae-detection-1/data.yaml',\n",
    "    epochs = 30,\n",
    "    batch = 16,\n",
    "    device=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
